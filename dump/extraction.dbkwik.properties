# download and extraction target dir
#base-dir= moved to $extraction-framework/core/src/main/resources/universal.properties

# Source file. If source file name ends with .gz or .bz2, it is unzipped on the fly. 
# Must exist in the directory xxwiki/yyyymmdd and have the prefix xxwiki-yyyymmdd-
# where xx is the wiki code and yyyymmdd is the dump date.

# default:
#source=# moved to $extraction-framework/core/src/main/resources/universal.properties

# use only directories that contain a 'download-complete' file? Default is false.
require-download-complete=true

# List of languages or article count ranges, e.g. 'en,de,fr' or '10000-20000' or '10000-', or '@mappings'
languages=@downloaded

# extractor class names starting with "." are prefixed by "org.dbpedia.extraction.mappings"

extractors=.ArticlePageExtractor,.ArticleCategoriesExtractor,.ArticleTemplatesExtractor,.CategoryLabelExtractor,\
.InfoboxExtractor,.LabelExtractor,.RedirectExtractor,.SkosCategoriesExtractor,\
.DisambiguationExtractor,.PageLinksExtractor,.ExternalLinksExtractor,.HomepageExtractor,.ImageExtractor,\
.TopicalConceptsExtractor,.AnchorTextExtractor,.InterLanguageLinksExtractor,.ArticleTemplatesClassExtractor,.NifSwebleExtractor,.TagExtractor

#.PageIdExtractor, .WikiPageLengthExtractor,.WikiPageOutDegreeExtractor

#.CitationExtractor, .PersondataExtractor

# If we need to Exclude Non-Free Images in this Extraction, set this to true
copyrightCheck=false


#parameters specific to the nif extraction:

#only extract abstract (not the whole page)
nif-extract-abstract-only=false
#the request query string
nif-query=&format=xml&action=parse&prop=text&page=%s&pageid=%d
#the xml path of the response
nif-tags=api,parse,text
# will leave out the long and short abstract datasets
nif-isTestRun=false
# will write all anchor texts for each nif instance
nif-write-anchor=false
# write only the anchor text for link instances
nif-write-link-anchor=true
